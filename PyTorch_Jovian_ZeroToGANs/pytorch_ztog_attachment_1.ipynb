{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasPequenoSterzeck/Data_Visualization/blob/main/PyTorch_Jovian_ZeroToGANs/pytorch_ztog_attachment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdZcnp_SQ0zX"
      },
      "source": [
        "# pytorch-ztog-attachment-1\n",
        "\n",
        "Here's the assingment that's I've recieve in first class of \"Deep Learning with PyTorch: Zero to GANs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DD-v6JwQ1jh"
      },
      "source": [
        "# Attachment 01 - PyTorch Zero to GANN's\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHFWKVg5Q8oR"
      },
      "source": [
        "## Questions for Review\n",
        "\n",
        "Try answering the following questions to test your understanding of the topics covered in this notebook:\n",
        "\n",
        "1. What is a linear regression model? Give an example of a problem formulated as a linear regression model.\n",
        ">Um problema de regressão linear é um problema aonde conseguimos traçar uma correlação entre as variáveis independentes de uma forma a correlacioná-las com a nossa variável alvo/target.<br>\n",
        "2. What are input and target variables in a dataset? Give an example.\n",
        "> **Input's:** São variáveis independentes que comportam o modelo (Ex: Ao avaliar uma colheita de cana de açucar temos como variáveis independentes (Input's) a umidade do solo, clima do período entre-safra, relação de pragas por hectar...)<br>**Target:** É a variável que queremos prever (Ex: Considerando uma colheita de cana podemos querer prever variáveis como tonelada por hectar ou nível de qualidade do açucar coletado da cana).<br>\n",
        "3. What are weights and biases in a linear regression model?\n",
        "> **Weights:** São os pesos que o modelo irá com muito esforço procurar encontrar entre as variáveis independentes e variável target, esses pesos tendem a iniciar de forma aleatória e conforme a função de custo escolhida e treinamento do modelo, os weights serão cada vez mais próximos da realidade de importancia das variáveis para a predição do objetivo.<br>**Biases:** É o ponto de partida de onde o modelo deve estipular a predição, digamos que em uma colhetia de cana não seja possível existir uma \"colheita negativa de cana\", o Bias irá nos ajudar com essa correção.\n",
        "4. How do you represent tabular data using PyTorch tensors?\n",
        "> Normalmente são representadas por meio de matrizes multidimensionais.\n",
        "5. Why do we create separate matrices for inputs and targets while training a linear regression model?\n",
        "> Basta declará-los de forma separada ou atribuí-los em variáveis diferentes, se forem importados via matrix Numpy fica ainda mais fácil utilizando o método slice com colchetes.\n",
        "6. How do you determine the shape of the weights matrix & bias vector given some training data?\n",
        "> Cada variável que queremos prever(target) terá seu conjunto de pesos que serão multiplicados pelas variáveis e então será somado o bias.\n",
        "7. How do you create randomly initialized weights & biases with a given shape?\n",
        "> basta utilizar torch.randn(shapre_of,tensor)\n",
        "8. How is a linear regression model implemented using matrix operations? Explain with an example.\n",
        "> Irei utilizar o exemplo fornecido em aula para melhor elucidar o cálculo..:\n",
        "![Imagem de exemplo](https://i.imgur.com/WGXLFvA.png)\n",
        "<br>Seria utilizado a operação de Matrix @ entre matrizes para a seguinte operação: 1ª Linha 'w1' : 73 * w11 + 67 * w12 + 43 * w13 + b1\n",
        "9. How do you generate predictions using a linear regression model?\n",
        "> As previsão são realizadas quando multiplicamos os pesos respectivos das variáveis independentes fornecidas e então adicionados bias para chegar ao valor que o modelo irá prever.\n",
        "10. Why are the predictions of a randomly initialized model different from the actual targets?\n",
        "> Porque os valores de weight's e Bia's que foram informados de forma aleatória dificilmente vão ser aqueles que melhor espelham a devida correlação das variáveis individuais para seu alvo.\n",
        "11. What is a loss function? What does the term “loss” signify?\n",
        "> Uma função de custo serve paa nos mostrar o quanto errado nosso modelo está, e também é o parâmetros base para conseguirmos quantificar o erro em relação a direção das variáveis(weights) que iremos alterar (acrescentar ou subtrair) para então minimizar o erro e chegar em previsões mais próximas do real.\n",
        "12. What is mean squared error?\n",
        "> Mean Squared Error ou Erro Médio Quadrático é uma medida utilizada quando se quer saber em um plano quadrático o quanto nossa previsão está longe do real cenário, de forma bem simplificada, ela funciona quando pegamos o valor real e o correlacionamos com o valor previsto do modelo.\n",
        "13. Write a function to calculate mean squared using model predictions and actual targets.\n",
        "> Feito em notebook auxiliar anterior.\n",
        "14. What happens when you invoke the `.backward` function on the result of the mean squared error loss function?\n",
        "> É usada para calcular os gradientes em relação aos parâmetros do modelo, permitindo a aplicação do algoritmo de otimização, como o Gradiente Descendente, para atualizar os pesos da rede neural durante o processo de treinamento.\n",
        "15. Why is the derivative of the loss w.r.t. the weights matrix itself a matrix? What do its elements represent?\n",
        "> A derivada da perda em relação à matriz de pesos é uma matriz porque ela captura a sensibilidade da perda em relação a cada peso na matriz. Seus elementos representam a contribuição de cada peso para a perda geral.\n",
        "16. How is the derivate of the loss w.r.t. a weight element useful for reducing the loss? Explain with an example.\n",
        "> A derivada da perda em relação a um elemento de peso indica como uma pequena mudança nesse peso afeta a perda. Ao ajustar o peso na direção oposta à sua derivada, podemos reduzir iterativamente a perda.\n",
        "17. Suppose the derivative  of the loss w.r.t. a weight element is positive. Should you increase or decrease the element’s value slightly to get a lower loss?\n",
        "> Se a derivada for positiva, diminuir o peso diminuirá a perda, e vice-versa.\n",
        "18. Suppose the derivative  of the loss w.r.t. a weight element is negative. Should you increase or decrease the element’s value slightly to get a lower loss?\n",
        "> Se a derivada for negativa, aumentar o peso diminuirá a perda, e vice-versa.\n",
        "19. How do you update the weights and biases of a model using their respective gradients to reduce the loss slightly?\n",
        "> Para darmos passo pequenos utilizamops uma taxa de aprendizagem bem pequena, dessa forma evitamos de dar passos grandes na direção errada.\n",
        "20. What is the gradient descent optimization algorithm? Why is it called “gradient descent”?\n",
        "> A descida de gradiente é a computação em sequẽncia do erro em sua descida ao custo da função objetivando que este seja cada vez menor, dito isso, quanto mais optimizado for nosso algoritmito de gradient descent mais rápido será a nossa \"descida\" para o custo mínimo de erro da função.\n",
        "21. Why do you subtract a “small quantity” proportional to the gradient from the weights & biases, not the actual gradient itself?\n",
        "> Conforme citado acima, para evitar que os passos que estamos tomando acabem levando o modelo para um caminho errado.\n",
        "22. What is learning rate? Why is it important?\n",
        "> É a taxa na qual iremos acatar a direção que a função de perca e descida de gradiente.\n",
        "23. What is `torch.no_grad`?\n",
        "> Desativa o cálculo automático de gradientes.\n",
        "24. Why do you reset gradients to zero after updating weights and biases?\n",
        "> Ao redefinir os gradientes para zero, garantimos que não haja acumulação indesejada de gradientes de iterações anteriores, permitindo um cálculo correto dos gradientes na próxima iteração. Essa etapa é fundamental para o bom funcionamento do algoritmo de otimização usado para treinar o modelo.\n",
        "25. What are the steps involved in training a linear regression model using gradient descent?\n",
        "> Realizar as operação de dados, pesos e bias, computar erro, computar gradiente, zerar gradiente, e então voltar ao começo...\n",
        "26. What is an epoch?\n",
        "> É uma única passagem completa pelo conjunto de treinamento, onde cada amostra do conjunto de dados é apresentada uma vez ao modelo.\n",
        "27. What is the benefit of training a model for multiple epochs?\n",
        "> Treinar um modelo por várias épocas permite que o modelo aprenda gradualmente padrões mais complexos nos dados e ajuste seus pesos de forma mais precisa, levando a uma melhor capacidade de generalização e desempenho do modelo.\n",
        "28. How do you make predictions using a trained model?\n",
        "> Para conseguir previsões utilizando um modelo somente precisamos multiplicar os pesos pelas variáveis independentes e somar o bias.\n",
        "29. What should you do if your model’s loss doesn’t decrease while training? Hint: learning rate.\n",
        "> Considerando que a dica foi learning rate, podemos pressupor que o modelo não está conseguindo reduzir a perca devido ao lerning rate atual estar muito alto ou ainda muito baixo, seria ideal alterá-lo e então observar o que ocorre com o custo da função.\n",
        "30. What is `torch.nn`?\n",
        "> É um módulo que fornece as ferramentas e classes necessárias para criar e treinar redes neurais\n",
        "31. What is the purpose of the `TensorDataset` class in PyTorch? Give an example.\n",
        "> Facilizar a segmentação da parte de treino e teste, além de manipular os dados de forma intuitiva e possibilitar também p shufle(aleatoriedade) na sequência dos dados.\n",
        "32. What is a data loader in PyTorch? Give an example.\n",
        "> Um data loader no PyTorch é um objeto que facilita o carregamento de dados em lotes durante o treinamento de modelos. Por exemplo, podemos criar um data loader para carregar um conjunto de dados de imagens, permitindo iterar sobre ele em lotes.\n",
        "33. How do you use a data loader to retrieve batches of data?\n",
        "> Para recuperar lotes de dados usando um data loader, podemos iterar sobre ele em um loop e obter um lote de cada vez. Por exemplo, usando um loop for batch in data_loader, podemos acessar os dados do lote dentro do loop.\n",
        "34. What are the benefits of shuffling the training data before creating batches?\n",
        "> Embaralhar os dados de treinamento antes de criar lotes ajuda a evitar que o modelo aprenda viéses indesejados ou padrões sequenciais e facilita a generalização para novos dados.\n",
        "35. What is the benefit of training in small batches instead of training with the entire dataset?\n",
        "> Treinar em lotes pequenos permite atualizações mais frequentes dos pesos do modelo, levando a uma convergência mais rápida e uso eficiente da memória.\n",
        "36. What is the purpose of the `nn.Linear` class in PyTorch? Give an example.\n",
        "> A classe **nn.Linear** é usada para definir uma **camada linear** em uma rede neural. Por exemplo, nn.Linear(10, 5) cria uma camada linear que mapeia 10 entradas para 5 saídas.\n",
        "37. How do you see the weights and biases of a `nn.Linear` model?\n",
        "> model.parameters()\n",
        "38. What is the purpose of the `torch.nn.functional` module?\n",
        "> Contém funções de ativação, funções de perda e outras operações comuns usadas na construção de redes neurais.\n",
        "39. How do you compute mean squared error loss using a PyTorch built-in function?\n",
        "> MSELoss()\n",
        "40. What is an optimizer in PyTorch?\n",
        "> Um optimizador será o cálculo que realizara a descida do gradiente.\n",
        "41. What is `torch.optim.SGD`? What does SGD stand for?\n",
        "> SGD é uma forma eficiente de realizar a descida de gradiente, seu significado é Stochastic Gradient Descent\n",
        "42. What are the inputs to a PyTorch optimizer?\n",
        "> Taxa de aprendizado é algo em comum, mas cada optimizador poderá oferecer mais critérios e configurações de acordo com seu objetivo.\n",
        "43. Give an example of creating an optimizer for training a linear regression model.\n",
        "> optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "44. Write a function to train a `nn.Linear` model in batches using gradient descent.\n",
        "> def train_model(model, dataloader, optimizer, loss_fn):\n",
        "        for inputs, labels in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "45. How do you use a linear regression model to make predictions on previously unseen data?\n",
        "> predicted_values = model(new_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora irei dedicar a escrita em inglês...\n",
        "\n",
        "# Top 5 must now tensor functions\n",
        "\n",
        "As we describe above, tensors are matrices that's we manipulate trow the code using the pytorch lib, so let's now more about some functions that can manipulate tensors"
      ],
      "metadata": {
        "id": "P96M3xAiXLMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's firs create 2 tensors for examples:\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "tA = torch.tensor([[1,2],\n",
        "                   [2,3],\n",
        "                   [3,4.]], requires_grad=True)\n",
        "\n",
        "tB = torch.tensor([[2,2],\n",
        "                   [2,2],\n",
        "                   [2,2.]], requires_grad=True)\n",
        "\n",
        "print(f'-> Tensors:\\n\\nTensor A: \\n{tA}\\n\\nTensor B: \\n{tB}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbSlmNMlXqCJ",
        "outputId": "82611e0d-f77e-45c9-9f99-a0aee335b90e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> Tensors:\n",
            "\n",
            "Tensor A: \n",
            "tensor([[1., 2.],\n",
            "        [2., 3.],\n",
            "        [3., 4.]], requires_grad=True)\n",
            "\n",
            "Tensor B: \n",
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------\n",
        "# [TORCH.TENSOR.POW](https://pytorch.org/docs/stable/generated/torch.Tensor.pow.html#torch.Tensor.pow)\n",
        "\n",
        "Tensor.pow(exponent)"
      ],
      "metadata": {
        "id": "fRepkboZZEhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor.pow is used to exponent calculations, such as below;\n",
        "\n",
        "tA.pow(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEPESFELZO7Z",
        "outputId": "20be6235-e6c6-427e-e00d-67403fc8b687"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.,  4.],\n",
              "        [ 4.,  9.],\n",
              "        [ 9., 16.]], grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tB.pow(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApKeyQjDZYDq",
        "outputId": "ea0b1f02-5a45-4fb1-bd56-4bac5c04e32d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8., 8.],\n",
              "        [8., 8.],\n",
              "        [8., 8.]], grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------\n",
        "# [TORCH.TENSOR.POW_](https://pytorch.org/docs/stable/generated/torch.Tensor.pow_.html#torch.Tensor.pow_)\n",
        "\n",
        "Tensor.pow_(exponent)\n",
        "\n",
        "In PyTorch, Tensor.pow() and Tensor.pow_() are two different methods for exponentiation."
      ],
      "metadata": {
        "id": "axiFWB__aCRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method returns a new tensor with each element raised to the power of the given exponent.\n",
        "The original tensor remains unchanged.\n",
        "\n",
        "Here's a simple example to illustrate the difference:"
      ],
      "metadata": {
        "id": "oRseGk1UaZRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tP = torch.tensor([2, 3, 4.])\n",
        "\n",
        "# Using Tensor.pow()\n",
        "result = tP.pow(2)\n",
        "print('Original tensor(tP) in POW function equals =', result)\n",
        "print('Original tesnor(tP) without any function equals =',tP)\n",
        "\n",
        "# Using Tensor.pow_()\n",
        "tP.pow_(2)\n",
        "print('\\nBut when we use the POW_ function, the original variable(tP) change the inner values in the exponencial =',tP)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvR_WWddaBwK",
        "outputId": "1c2214d3-369f-4b59-d388-90d276b1bc17"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor(tP) in POW function equals = tensor([ 4.,  9., 16.])\n",
            "Original tesnor(tP) without any function equals = tensor([2., 3., 4.])\n",
            "\n",
            "But when we use the POW_ function, the original variable(tP) change the inner values in the exponencial = tensor([ 4.,  9., 16.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE NOTE!** To use pow_ we cannot use tesnors with requires_grad=True."
      ],
      "metadata": {
        "id": "HPqiqTVmbfGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------\n",
        "# [TORCH.TENSOR.QSCHEME](https://pytorch.org/docs/stable/generated/torch.Tensor.qscheme.html#torch.Tensor.qscheme)\n",
        "\n",
        "Tensor.qscheme() → torch.qscheme\n",
        "\n",
        "Returns the quantization scheme of a given QTensor.\n",
        "\n",
        "Importante Note about this function: The qscheme() method is specifically designed to retrieve the quantization scheme of a quantized tensor (QTensor). In order to use qscheme(), you need to have a tensor that has undergone quantization using PyTorch's quantization functions.\n",
        "\n",
        "So befor we use the QSCHEME() we need to use:\n",
        "## [TORCH.TENSOR.QUANTIZE_PER_TENSOR](https://pytorch.org/docs/stable/generated/torch.quantize_per_tensor.html)\n",
        "The torch.quantize_per_tensor() function is one of the methods available in PyTorch to quantize a tensor. It applies per-tensor quantization by determining a scaling factor (scale) and zero point (zero_point) to represent the tensor's values within a reduced range.\n",
        "> In the practive: torch.quantize_per_tensor just turn each value into INT type"
      ],
      "metadata": {
        "id": "rIofvgTgbyyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize the tensor\n",
        "scale = 0.1\n",
        "zero_point = 0\n",
        "quantized_tensor = torch.quantize_per_tensor(tA, scale, zero_point, dtype=torch.quint8)\n",
        "\n",
        "# Get the quantization SCHEME\n",
        "qscheme = quantized_tensor.qscheme()\n",
        "print(qscheme)  # Output: torch.per_tensor_affine\n",
        "\n",
        "# Additional operations based on the quantization scheme\n",
        "if qscheme == torch.per_tensor_affine:\n",
        "    print(\"This is a per-tensor affine quantized tensor.\")\n",
        "elif qscheme == torch.per_channel_affine:\n",
        "    print(\"This is a per-channel affine quantized tensor.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-7AKCGlbd_U",
        "outputId": "e4ba9fd0-990e-490b-f9b4-8be6d5dfa4cc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.per_tensor_affine\n",
            "This is a per-tensor affine quantized tensor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** \"affine\" indicates that the quantization scheme uses an affine transformation to map floating-point values to quantized integer values"
      ],
      "metadata": {
        "id": "K3TO982ggDAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------\n",
        "# [TORCH.TENSOR.RESHAPE](https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html#torch.Tensor.reshape)\n",
        "\n",
        "Tensor.reshape(*shape) → Tensor\n",
        "\n",
        "Returns a tensor with the same data and number of elements as self but with the specified shape. This method returns a view if shape is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view."
      ],
      "metadata": {
        "id": "pVlSNJQ4gGzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tX = tA.reshape([2,3])\n",
        "\n",
        "print(f'This is the original tensor(tA)\\n{tA}\\nWith Shape: {tA.shape}\\n\\nThis is the new reshaped tensor(tX):\\n{tX}\\nWith the new shape: {tX.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAlfsYvigWz1",
        "outputId": "f76bfba1-1432-4e21-8cdd-bef510aff4ab"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the original tensor(tA)\n",
            "tensor([[1., 2.],\n",
            "        [2., 3.],\n",
            "        [3., 4.]], requires_grad=True)\n",
            "With Shape: torch.Size([3, 2])\n",
            "\n",
            "This is the new reshaped tensor(tX):\n",
            "tensor([[1., 2., 2.],\n",
            "        [3., 3., 4.]], grad_fn=<ReshapeAliasBackward0>)\n",
            "With the new shape: torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Other apliccation in reshape():\n",
        "\n",
        "tY = tB.reshape([6,1])\n",
        "\n",
        "print(f'This is the original tensor(tB)\\n{tB}\\nWith Shape: {tB.shape}\\n\\nThis is the new reshaped tensor(tY):\\n{tY}\\nWith the new shape: {tY.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm6_xshag03O",
        "outputId": "73ab4ce9-aa3f-4782-ebc9-b5ceb77171e2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the original tensor(tB)\n",
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]], requires_grad=True)\n",
            "With Shape: torch.Size([3, 2])\n",
            "\n",
            "This is the new reshaped tensor(tY):\n",
            "tensor([[2.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [2.]], grad_fn=<ReshapeAliasBackward0>)\n",
            "With the new shape: torch.Size([6, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------\n",
        "# [TORCH.TENSOR.ROUND](https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round)\n",
        "\n",
        "Tensor.round(decimals=0) → Tensor\n",
        "\n",
        "Simply round the values of the tensor, let's aplly:"
      ],
      "metadata": {
        "id": "1PfBQvZVhCOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ten = torch.tensor([1.154,2.895,6.748496])\n",
        "print('Original tensor: ', ten)\n",
        "\n",
        "tenn = ten.round(decimals=2)\n",
        "print('\\nNew tensor: ', tenn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAuY7cuKhRwm",
        "outputId": "8270d89a-b347-4e4d-98e8-e6ba1e227539"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor:  tensor([1.1540, 2.8950, 6.7485])\n",
            "\n",
            "New tensor:  tensor([1.1500, 2.9000, 6.7500])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------\n",
        "# [TORCH.TENSOR.SPLIT](https://pytorch.org/docs/stable/generated/torch.Tensor.split.html#torch.Tensor.split)\n",
        "\n",
        "Tensor.split(split_size, dim=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "1UyUSvmRhyGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "print('Original tensor: ',tensor,'\\n')\n",
        "\n",
        "# Split the tensor into chunks of size 3\n",
        "chunks = torch.split(tensor, 3)\n",
        "\n",
        "# Print the resulting chunks\n",
        "for i,chunk in enumerate(chunks, start=1):\n",
        "    print(f\"{i}º Chunk: {chunk}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Osg7FDuiiEo2",
        "outputId": "8af70720-432c-4b0d-dc98-b7b3eb68d6a0"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor:  tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]) \n",
            "\n",
            "1º Chunk: tensor([1, 2, 3])\n",
            "2º Chunk: tensor([4, 5, 6])\n",
            "3º Chunk: tensor([7, 8, 9])\n",
            "4º Chunk: tensor([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's aplly the same function but in tensor B:\n",
        "print('Original tensor(tB): \\n',tB,'\\n')\n",
        "\n",
        "# Split the tensor into chunks of size 1 along dimension 0\n",
        "chunks = torch.split(tB, 1, dim=0)\n",
        "\n",
        "# Print the resulting chunks\n",
        "for i,chunk in enumerate(chunks, start=1):\n",
        "    print(f\"{i}º Chunk: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdSB7eZTi_m_",
        "outputId": "bf2fbaab-6d6d-4f22-8710-23b6bc05d48d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor(tB): \n",
            " tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]], requires_grad=True) \n",
            "\n",
            "1º Chunk: tensor([[2., 2.]], grad_fn=<SplitBackward0>)\n",
            "2º Chunk: tensor([[2., 2.]], grad_fn=<SplitBackward0>)\n",
            "3º Chunk: tensor([[2., 2.]], grad_fn=<SplitBackward0>)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}